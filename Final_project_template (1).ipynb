{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title:Spam Classification Using Naive Bayes with Parameter Optimization\n",
    "\n",
    "\n",
    "#### Group Member Names :Harsh Patel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION: \n",
    "The rise in unwanted messages on social media and email has made spam detection increasingly critical. Spammers exploit platforms like Facebook, Twitter, and YouTube to disseminate malware, phishing links, and deceptive content. Spam, defined as unsolicited digital communication, poses significant risks to users' safety and platform security. Research highlights the use of text-based techniques, machine learning, and deep learning for spam identification while addressing challenges related to datasets and detection methods. Despite efforts, spam continues to grow, causing financial losses for both companies and individuals.\n",
    "*********************************************************************************************************************\n",
    "#### AIM : \n",
    "This project aims to classify spam messages using the Naive Bayes algorithm, an effective method for text classification tasks. The study compares two popular text vectorization techniques, CountVectorizer and TfidfVectorizer, to identify the most suitable approach. A significant contribution is made by optimizing the Naive Bayes hyperparameters (alpha, ngram_range, and max_df) using GridSearchCV. The results demonstrate a marked improvement in model performance through hyperparameter tuning, as measured by metrics such as accuracy, precision, recall, F1-score, and AUC.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo: https://github.com/Harshhpp/Harsh.Patel.BDAT1004PS1.git\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "The paper explores the challenge of spam email detection, focusing on the limitations of existing spam filters like Gmail's system. It highlights how legitimate emails can sometimes be misclassified as spam, and vice versa. The research investigates the storage of user data in browsers (Internet Explorer, Firefox, and Chrome on Windows 10) and uses machine learning algorithms, particularly the KN and NB models, to improve spam detection accuracy. The study also introduces two filtering models, Opinion Rank and Latent Dirichlet Allocation (LDA), to enhance the classification of spam emails. The research aims to offer better solutions for managing the growing volume of spam emails and to contribute to the development of more effective spam filtering techniques using machine learning.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "Trust in social systems evolves over time as individuals gain life experience and engage with diverse social networks. However, current models often overlook the dynamics of trust and fail to account for multiple languages, which is crucial due to the global nature of social platforms. Additionally, linguistic spam may lead to the disregard of valuable information. Future research should focus on incorporating trust dynamics, multilingual data, and multimedia content to enhance the accuracy and applicability of trust models across various domains.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "The context of the problem revolves around the evolving nature of trust in social systems, particularly within online platforms like social networks. As individuals interact with these platforms, their trust in the system fluctuates based on their experiences and social engagements. Traditional trust models often fail to account for this dynamic shift, as well as the multilingual and multimedia aspects of user-generated content. Given the global nature of social networks, these models typically focus on textual tags and user profiles, ignoring the value of other forms of communication (audio, visual) and the challenges posed by linguistic spam. The problem lies in the need for more effective trust modeling that incorporates these complexities, ensuring that trust dynamics are better understood and applied across different languages, cultures, and types of content.\n",
    "*\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION: \n",
    "The solution involves implementing a text-based Machine Learning pipeline using Multinomial Naive Bayes for spam classification. By preprocessing text data and leveraging feature extraction techniques, the model identifies spam messages with high accuracy. To enhance performance, hyperparameter tuning and ROC-AUC evaluation ensure robust detection. This approach provides an efficient, scalable, and adaptable solution to the spam detection problem.\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "\n",
    "# Chart - 1 Pie Chart Visualization Code For Distribution of Spam vs Ham Messages\n",
    "spread = df['Category'].value_counts()\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "# Set Labels\n",
    "spread.plot(kind = 'pie', autopct='%1.2f%%', cmap='Set1')\n",
    "plt.title(f'Distribution of Spam vs Ham')\n",
    "\n",
    "# Display the Chart\n",
    "plt.show()\n",
    "     \n",
    "# Splitting Spam Messages\n",
    "df_spam = df[df['Category']=='spam'].copy()\n",
    "# Chart - 2 WordCloud Plot Visualization Code For Most Used Words in Spam Messages\n",
    "# Create a String to Store All The Words\n",
    "comment_words = ''\n",
    "\n",
    "# Remove The Stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Iterate Through The Column\n",
    "for val in df_spam.Message:\n",
    "\n",
    "    # Typecaste Each Val to String\n",
    "    val = str(val)\n",
    "\n",
    "    # Split The Value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts Each Token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "# Set Parameters\n",
    "wordcloud = WordCloud(width = 1000, height = 500,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10,\n",
    "                max_words = 1000,\n",
    "                colormap = 'gist_heat_r').generate(comment_words)\n",
    "\n",
    "# Set Labels\n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.title('Most Used Words In Spam Messages', fontsize = 15, pad=20)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "# Display Chart\n",
    "plt.show()\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    '''The function will take model, x train, x test, y train, y test\n",
    "    and then it will fit the model, then make predictions on the trained model,\n",
    "    it will then print roc-auc score of train and test, then plot the roc, auc curve,\n",
    "    print confusion matrix for train and test, then print classification report for train and test,\n",
    "    then plot the feature importances if the model has feature importances,\n",
    "    and finally it will return the following scores as a list:\n",
    "    recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test\n",
    "    '''\n",
    "\n",
    "    # fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the test data\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    pred_prob_train = model.predict_proba(X_train)[:,1]\n",
    "    pred_prob_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # calculate ROC AUC score\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "    print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
    "    print(\"Test ROC AUC:\", roc_auc_test)\n",
    "\n",
    "    # plot the ROC curve\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
    "    plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
    "    plt.legend()\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "\n",
    "    # calculate confusion matrix\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(11,4))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[0])\n",
    "    ax[0].set_xlabel(\"Predicted Label\")\n",
    "    ax[0].set_ylabel(\"True Label\")\n",
    "    ax[0].set_title(\"Train Confusion Matrix\")\n",
    "\n",
    "    sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[1])\n",
    "    ax[1].set_xlabel(\"Predicted Label\")\n",
    "    ax[1].set_ylabel(\"True Label\")\n",
    "    ax[1].set_title(\"Test Confusion Matrix\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # calculate classification report\n",
    "    cr_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    cr_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    print(\"\\nTrain Classification Report:\")\n",
    "    crt = pd.DataFrame(cr_train).T\n",
    "    print(crt.to_markdown())\n",
    "    # sns.heatmap(pd.DataFrame(cr_train).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    crt2 = pd.DataFrame(cr_test).T\n",
    "    print(crt2.to_markdown())\n",
    "    # sns.heatmap(pd.DataFrame(cr_test).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
    "\n",
    "\n",
    "    precision_train = cr_train['weighted avg']['precision']\n",
    "    precision_test = cr_test['weighted avg']['precision']\n",
    "\n",
    "    recall_train = cr_train['weighted avg']['recall']\n",
    "    recall_test = cr_test['weighted avg']['recall']\n",
    "\n",
    "    acc_train = accuracy_score(y_true = y_train, y_pred = y_pred_train)\n",
    "    acc_test = accuracy_score(y_true = y_test, y_pred = y_pred_test)\n",
    "\n",
    "    F1_train = cr_train['weighted avg']['f1-score']\n",
    "    F1_test = cr_test['weighted avg']['f1-score']\n",
    "\n",
    "    model_score = [precision_train, precision_test, recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test ]\n",
    "    return model_score\n",
    "    # Defining a function for the Email Spam Detection System\n",
    "def detect_spam(email_text):\n",
    "    # Load the trained classifier (clf) here\n",
    "    # Replace the comment with your code to load the classifier model\n",
    "\n",
    "    # Make a prediction using the loaded classifier\n",
    "    prediction = clf.predict([email_text])\n",
    "\n",
    "    if prediction == 0:\n",
    "        return \"This is a Ham Email!\"\n",
    "    else:\n",
    "        return \"This is a Spam Email!\"\n",
    "# Example of how to use the function\n",
    "sample_email = 'Free Tickets for IPL'\n",
    "result = detect_spam(sample_email)\n",
    "print(result)\n",
    "*********************************************************************************************************************\n",
    "\n",
    "*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code : # Function to Evaluate Model\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    pred_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Classification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curve  \n",
    "    fpr, tpr, _ = roc_curve(y_test, pred_prob_test)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_test, pred_prob_test):.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # 1: CountVectorizer with Default MultinomialNB\n",
    "print(\"**Baseline Model: CountVectorizer + Default MultinomialNB**\")\n",
    "pipeline_count = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "evaluate_model(pipeline_count, X_train, X_test, y_train, y_test)\n",
    "# 2: TfidfVectorizer with Default MultinomialNB\n",
    "print(\"\\n**Baseline Model: TfidfVectorizer + Default MultinomialNB**\")\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "evaluate_model(pipeline_tfidf, X_train, X_test, y_train, y_test)\n",
    "# Import GridSearchCV \n",
    "from sklearn.model_selection import GridSearchCV # Importing GridSearchCV\n",
    "\n",
    "pipeline_optimized = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],  \n",
    "    'vectorizer__max_df': [0.75, 1.0],           \n",
    "    'classifier__alpha': [0.1, 0.5, 1.0]         \n",
    "}\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(pipeline_optimized, param_grid, cv=3, scoring='roc_auc', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Best Parameters and Score\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation AUC Score:\", grid_search.best_score_)\n",
    "# Evaluate Best Model\n",
    "best_model = grid_search.best_estimator_\n",
    "evaluate_model(best_model, X_train, X_test, y_train, y_test)\n",
    "# ML Model - 1 Implementation\n",
    "# Create a machine learning pipeline using scikit-learn, combining text vectorization (CountVectorizer)\n",
    "# and a Multinomial Naive Bayes classifier for email spam detection.\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),  # Step 1: Text data transformation\n",
    "    ('nb', MultinomialNB())  # Step 2: Classification using Naive Bayes\n",
    "])\n",
    "\n",
    "# Model is trained (fit) and predicted in the evaluate model\n",
    "# Visualizing evaluation Metric Score chart\n",
    "MultinomialNB_score = evaluate_model(clf, X_train, X_test, y_train, y_test)\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "Model performance: baseline models should perform relatively well, with TfidfVectorizer-based models consistently outperforming CountVectorizer-based models.\n",
    "AUC Score: Optimizing the model using grid search should result in a higher AUC score, indicating improved classification accuracy.\n",
    "Confusion Matrix: A good model should have a high True Positive and True Negative with a low False Positive and False Negative.\n",
    "ROC Curve: An ideal model will have a high AUC, indicating that it can distinguish between classes successfully.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "Model Performance: Using the evaluation metrics, determine how well the model is performing. \n",
    "Feature Engineering influence: Consider the influence of feature extraction approaches (for example, CountVectorizer vs. TfidfVectorizer). \n",
    "Hyperparameter Tuning: The grid search can help locate the optimum hyperparameters for fine-tuning the model. It could demonstrate that tweaking n-grams or alpha (for Naive Bayes) considerably increases performance.\n",
    "Cross-validation and overfitting: Evaluate performance on both the training and test sets to see if there is a substantial gap (overfitting) or consistent performance (excellent generalization).\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction:\n",
    "This study examined the implementation and evaluation of multiple machine learning models for classification tasks, with an emphasis on text classification utilizing Naive Bayes classifiers and vectorization approaches (CountVectorizer and TfidfVectorizer). The models were evaluated using key measures such as precision, recall, F1 score, AUC score, and confusion matrices. The results show that TfidfVectorizer with Naive Bayes outperforms CountVectorizer in terms of AUC scores and model performance following grid search optimization. The revised model improves classification accuracy and is ideal for text classification tasks such as spam detection or sentiment analysis.\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "Understanding Text Representation: Learn how alternative vectorization strategies (CountVectorizer vs TfidfVectorizer) affect text classification performance.\n",
    "Model Optimization: The grid search demonstrates the value of hyperparameter adjustment and model optimization in improving model performance.\n",
    "Model Evaluation: How metrics such as AUC, accuracy, recall, and confusion matrices aid in assessing model performance and capacity to generalize effectively to previously encountered data.\n",
    "Practical Application: How the Naive Bayes classifier works well for text classification problems, especially when combined with the appropriate preprocessing steps.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "The model evaluation results showed that TfidfVectorizer + MultinomialNB outperformed CountVectorizer + MultinomialNB in terms of precision, recall, and AUC scores. This suggests that adding more weight to key phrases in the document, like Tfidf does, improves the model's ability to distinguish between classes. The grid search optimization also improved the model's performance by altering parameters like the n-gram range and smoothing factor (alpha). The confusion matrix and ROC curves offered visual information about the model's ability to accurately classify, with the optimized model displaying fewer misclassifications.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "Data Dependency: The caliber and volume of the training data have a significant impact on the models' performance. Poor generalization to unknown data or overfitting might result from incomplete or biased data.\n",
    "Interpretability: Although Naive Bayes classifiers can be understood, deep learning models or ensemble approaches may be better at identifying intricate patterns in the data.\n",
    "Text Preprocessing: Using more sophisticated preprocessing methods (such lemmatization, stop word removal, etc.) may enhance the models' performance.\n",
    "Multilingual Handling: The models' efficacy in a worldwide setting may be limited because they fail to take into consideration the dataset's linguistic diversity or various languages.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "Deep Learning Models: Going beyond conventional models, future research may use deep learning methods for more complex text understanding and categorization, such as transformers, recurrent neural networks (RNNs), or BERT.\n",
    "Multilingual Models: Considering the worldwide scope of social networks, it may be beneficial to integrate multilingual text data and investigate multilingual vectorization techniques such as multilingual BERT.\n",
    "Reducing Noise: The robustness and performance of the model would be enhanced by using improved methods for removing spam or noise from the textual input.\n",
    "Domain-Specific Tuning: Improving classification accuracy in particular domains by fine-tuning the models for those domains (such as legal papers or medical texts).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References: https://www.kaggle.com/code/anaghakp/email-spam-detection/notebook\n",
    "\n",
    "[1]:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
